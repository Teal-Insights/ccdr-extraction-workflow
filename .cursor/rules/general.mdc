---
description: 
globs: 
alwaysApply: true
---
This repository contains the extraction workflow for scraping World Bank Country and Climate Development Reports (CCDRs) from the Internet. This is part of the data preparation pipeline for the [CCDR Explorer](mdc:README.md), created by Teal Insights for Nature Finance.

The repository focuses specifically on:

1. Scraping CCDR publication metadata from the World Bank Open Knowledge Repository
2. Downloading PDF files
3. Uploading metadata to a PostgreSQL database
4. Uploading PDFs to OpenAI vector store for AI-powered search

Working documents and helper files not meant to be retained may be placed in the `artifacts` folder, which is ignored by git.

To avoid confusion, keep your bash console opened to the project root and specify all file paths relative to root.

# Workflow

There are currently at least 67 CCDRs (including two Tajikistan reports with the same title), comprising on the order of 200 PDF files. Additional CCDRs are published regularly, so ultimately we will need to automate the end-to-end workflow to continually ingest newly published documents.

The extraction workflow is organized in the `extract` folder and can be run as a complete pipeline using [extract_ccdrs.py](mdc:extract_ccdrs.py) or as individual steps.

## Extraction Steps

The extraction workflow consists of 9 sequential steps:

1. **Extract Publication Links** - Scrapes publication links from the World Bank repository, creating `data/publication_links.json`
2. **Extract Publication Details** - Extracts detailed information from each publication page, creating `data/publication_details.json`
3. **Add IDs** - Adds unique IDs to publications and download links
4. **Classify File Types** - Classifies file types for each download link using LLM
5. **Filter Download Links** - Filters and classifies which links to download
6. **Download Files** - Downloads the selected PDF files to `data/pub_*/doc_*.pdf`
7. **Convert BIN Files** - Converts .bin files to .pdf if they are PDF documents
8. **Upload to Database** - Uploads publications and documents to PostgreSQL database using [schema.py](mdc:extract/schema.py)
9. **Upload PDFs to OpenAI** - Uploads PDF files to OpenAI vector store for AI-powered search

Each CCDR ("publication") may consist of one or more PDF files ("documents"). The workflow enriches publication data with scraped details, generated IDs, and LLM-generated document classifications.

## Usage

Run the complete workflow:
```bash
uv run extract_ccdrs.py
```

Or run individual steps:
```bash
uv run -m extract.extract_publication_links
uv run -m extract.extract_publication_details
# ... etc
```

## Document Content Processing

Note that this repository handles extraction and basic metadata processing only. The detailed parsing and transformation of document content (text extraction, section identification, hierarchical structuring, etc.) happens in separate repositories as part of the broader CCDR analysis pipeline.

- This project uses `uv` to manage dependencies and run Python files.

Add Python dependencies like:

```bash
uv add pydantic # add --dev flag for development dependencies
```

Then run the file with `uv run`, like:

```bash
uv run -m extract.extract_publication_links
```

